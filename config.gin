import trax.layers
import trax.models
import trax.optimizers
import trax.data.inputs
import trax.supervised.trainer_lib

# Parameters that will vary between experiments:
# ==============================================================================
train.model = @trax.models.ReformerLM
# Our model will have 6 layers, alternating between the LSH attention proposed
# in the Reformer paper and local attention within a certain context window.
n_layers = 8
n_heads = 8
attn_type = [
  @trax.layers.SelfAttention,
  @trax.layers.SelfAttention,
  @LSHSelfAttention,
  @trax.layers.SelfAttention,
  ]
share_qk = False  # LSH attention ignores this flag and always shares q & k
vocab_size = 32768
attn_kv = 64
dropout = 0.2
n_tokens = 2048

# Parameters for multifactor:
# ==============================================================================
multifactor.constant = 0.0003
multifactor.warmup_steps = 10000
multifactor.factors = 'constant * linear_warmup * rsqrt_decay'

# Parameters for Adafactor:
# ==============================================================================
Adafactor.beta1 = 0.0
Adafactor.clipping_threshold = 1.0
Adafactor.decay_rate = 0
Adafactor.epsilon1 = 1e-30
Adafactor.epsilon2 = 0.001
Adafactor.factored = True

# Parameters for SelfAttention:
# ==============================================================================
trax.layers.SelfAttention.attention_dropout = %dropout
trax.layers.SelfAttention.chunk_len = 256
trax.layers.SelfAttention.n_chunks_before = 1
trax.layers.SelfAttention.n_parallel_heads = 1
trax.layers.SelfAttention.n_heads = %n_heads

# Parameters for LSHSelfAttention:
# ==============================================================================
LSHSelfAttention.attention_dropout = 0.0
LSHSelfAttention.chunk_len = 256
LSHSelfAttention.n_buckets = 512
LSHSelfAttention.n_chunks_after = 0
LSHSelfAttention.n_chunks_before = 1
LSHSelfAttention.n_hashes = 2
LSHSelfAttention.n_parallel_heads = 1
LSHSelfAttention.predict_drop_len = 256
LSHSelfAttention.predict_mem_len = 1024
LSHSelfAttention.n_parallel_heads = 1
LSHSelfAttention.n_heads = %n_heads

# Parameters for ReformerLM:
# ==============================================================================
ReformerLM.vocab_size = %vocab_size
ReformerLM.attention_type = %attn_type
ReformerLM.d_attention_key = %attn_kv
ReformerLM.d_attention_value = %attn_kv
ReformerLM.d_model = 512
ReformerLM.d_ff = 2048
ReformerLM.dropout = %dropout
ReformerLM.ff_activation = @trax.layers.Relu
ReformerLM.max_len = %n_tokens
ReformerLM.mode = 'train'
ReformerLM.n_heads = %n_heads
ReformerLM.n_layers = %n_layers
ReformerLM.axial_pos_shape = (32, 64)
ReformerLM.d_axial_pos_embs= (256, 256)