import trax.layers
import trax.models
import trax.optimizers
import trax.data.inputs
import trax.supervised.trainer_lib

# Parameters that will vary between experiments:
# ==============================================================================
train.model = @trax.models.Reformer2
# Our model will have 6 layers, alternating between the LSH attention proposed
# in the Reformer paper and local attention within a certain context window.
n_layers = 6
n_heads = 8
attn_type = [
  @trax.layers.SelfAttention,
  @LSHSelfAttention,  
  @trax.layers.SelfAttention,
  @LSHSelfAttention,
  @trax.layers.SelfAttention,
  @LSHSelfAttention,
  ]
share_qk = False  # LSH attention ignores this flag and always shares q & k
vocab_size = 32768
attn_kv = 64
dropout = 0.1
n_tokens = 4096

# Parameters for multifactor:
# ==============================================================================
multifactor.constant = 0.001
multifactor.warmup_steps = 2000
multifactor.factors = 'constant * linear_warmup * rsqrt_decay'

# Parameters for Adafactor:
# ==============================================================================
Adafactor.beta1 = 0.0
Adafactor.clipping_threshold = 1.0
Adafactor.decay_rate = None
Adafactor.epsilon1 = 1e-30
Adafactor.epsilon2 = 0.001
Adafactor.factored = True

# Parameters for SelfAttention:
# ==============================================================================
trax.layers.SelfAttention.attention_dropout = %dropout
trax.layers.SelfAttention.chunk_len = 64
trax.layers.SelfAttention.n_chunks_before = 1
trax.layers.SelfAttention.n_parallel_heads = 4
trax.layers.SelfAttention.n_heads = %n_heads

# Parameters for LSHSelfAttention:
# ==============================================================================
LSHSelfAttention.attention_dropout = 0.0
LSHSelfAttention.chunk_len = 64
LSHSelfAttention.n_buckets = [256, 512]
LSHSelfAttention.n_chunks_after = 0
LSHSelfAttention.n_chunks_before = 1
LSHSelfAttention.n_hashes = 2
LSHSelfAttention.n_parallel_heads = 1
LSHSelfAttention.predict_drop_len = 128
LSHSelfAttention.predict_mem_len = 1024
LSHSelfAttention.n_parallel_heads = 4
LSHSelfAttention.n_heads = %n_heads

# Parameters for Reformer2:
# ==============================================================================
Reformer2.input_vocab_size = %vocab_size
Reformer2.output_vocab_size = %vocab_size
Reformer2.encoder_attention_type = trax.layers.SelfAttention
Reformer2.encoder_decoder_attention_type  = LSHSelfAttention
Reformer2.d_attention_key = %attn_kv
Reformer2.d_attention_value = %attn_kv
Reformer2.d_model = 512
Reformer2.d_ff = 2048
Reformer2.dropout = %dropout
Reformer2.ff_activation = @trax.layers.Relu
Reformer2.max_len = %n_tokens
Reformer2.mode = 'train'
Reformer2.n_heads = %n_heads
Reformer2.n_decoder_layers = %n_layers
Reformer2.n_encoder_layers = %n_layers
Reformer2.axial_pos_shape = (64, 64)
Reformer2.d_axial_pos_embs= (256, 256)